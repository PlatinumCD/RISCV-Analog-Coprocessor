\documentclass[sigconf, review]{acmart}


\newcommand{\mat}[1]{\mathbf{#1}}
\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}

\title{Simulating Hybrid Analog + RISC-V Systems for HPC Applications}

\author{Cameron Durbin}
\authornote{This work was conducted during an internship at Sandia National Laboratories}
\affiliation{%
  \institution{University of Oregon}
  \city{Eugene}
  \state{Oregon}
  \country{USA}
}
\email{cfd@uoregon.edu}

\author{Jacob Flores}
\affiliation{%
  \institution{Sandia National Laboratories}
  \city{Albuquerque}
  \state{New Mexico}
  \country{USA}
}
\email{jmflore@sandia.gov}

\author{Thomas Weatherly}
\authornotemark[1]
\affiliation{%
  \institution{Georgia Tech Research Institute}
  \city{Atlanta}
  \state{Georgia}
  \country{USA}
}
\email{Thomas.Weatherly@gtri.gatech.edu}

\author{Ben Feinberg}
\affiliation{%
  \institution{Sandia National Laboratories}
  \city{Albuquerque}
  \state{New Mexico}
  \country{USA}
}
\email{bfeinbe@sandia.gov}


\begin{abstract}

%\begin{itemize}
%    \item HPC is used across many domains.
%    \item To speedup HPC applications, accelerators are used.
%    \item Accelerators consume an enormous amount of power performing linear operations. 
%    \item Researchers are seeking new methods to compute at lower power.
%    \item Memristors have revived interest in analog crossbars, a low power semiconductor device for computing mvm.
%    \item Simulation offers insight into the performance and accuracy of novel architectures before being developed.
%    \item We assembled a simulation platform that enables researchers to experiment with analog architectures.
%    \item We wrote iterative linear solvers that are executed on our platform.
%    \item We run our programs through on out platform and provide performance metrics.
%\end{itemize}

As digital scaling trends have slowed over the past decade, there has been renewed interest in new computing paradigms such as analog.
Analog computing has the potential to provide performance and efficiency beyond what is achievable by digital systems; however, many challenges remain.
One such challenge is supporting complex applications using analog components that implement few computational kernels.
We consider on a class of hybrid analog + digital systems where analog accelerators are used as tightly integrated coprocessors within each core.
The RISC-V ISA simplifies the design of hybrid systems, providing a mature software stack for the digital components allowing system designers to focus on the analog-specific aspects of the architecture and software.
To investigate the viability of these architectures for high performance computing we evaluate two iterative linear solvers using hybrid analog + RISC-V processors using the Structural Simulation Toolkit.
% TODO Results sentence. 

% High-Performance Computing (HPC) is vital across a wide range of scientific and engineering domains.
% Modern systems achieve their performance gains through specialized hardware accelerators, but this often comes at the cost of substantial power draw, particularly for dense operations such as matrix-vector multiplications (MVMs).
% As energy efficiency becomes an increasingly critical constraint, researchers are exploring new hardware paradigms to compute MVMs at reduced energy costs.
% One promising direction is the use of analog crossbars-resistive memory arrays that naturally perform MVMs using Ohm's law.

% To investigate the viability of analog-digital hybrid architectures in HPC, we extended SST's RISC-V CPU model to support a RoCC-connected analog coprocessor.
% This platform enables exploration of realistic hardware behaviors, software interfaces, and execution patterns in an analog-digital hybrid environment.
% We demonstrate the platform's capability by running iterative linear solvers and reporting performance metrics that highlight trade-offs between accuracy, latency, and analog complexity.
% Our results offer early insight into the opportunities and challenges of integrating analog acceleration into future HPC architectures.

\end{abstract}

\keywords{RISC-V, High-Performance Computing, Emerging Memories}

\maketitle
\input{intro}
\input{background}
\input{hardware}
\input{software}


\section{Evaluation}


To evaluate our system architecture, we selected two widely-used iterative linear solvers CG and BiCG-Stab.
Importantly, although iterative linear solvers are typically used for sparse matrices, to focus the analysis in this work we use a pair of synthetic dense matrices.
The convergence of the solvers was verified on smaller test cases such that only 10 iterations were simulated for this larger benchmarking effort.
Additionally, we assume that the floating-point emulation from prior work avoids additional iterations caused be analog imprecision.
Co-simulation of both analog precision and performance is an important topic for future work.

\subsection{Experimental Setup}

Our first experiment measures how the runtime of CG and BiCG-Stab scales with increasing problem size $N$, comparing single core executions with and without a single analog accelerator.
In this analysis the tile has as co-processor array size of $64\times64$ such that only a single array is needed.
This comparison allows us to quantify the accelerator’s contribution to performance and identify the scaling behavior of each solver.
We vary $N$ over a range in multiples of two. 
The results provide insight into how hardware acceleration affects runtime efficiency.


Our second experiment uses a fixed-size $1024\times1024$ dense matrix with $128\times128$ analog MMV arrays. This means a total of 64 arrays are required for the full application, which we consider in two hardware configurations:
\begin{enumerate}
    \item \textbf{Single-tile system} with 64 compute arrays
    \item \textbf{Two-tile system} with 32 compute arrays per tile
\end{enumerate}

Each configuration is tested under two pipeline designs:
\begin{itemize}
    \item \textbf{Small pipeline} --- 32\,KB L1 caches, 256\,KB L2 cache, 2-wide pipeline two floating-point units (FPUs), two integer units (IUs), and a 64-entry reorder buffer (ROB)
    \item \textbf{Large pipeline} --- 32\,KB L1 caches,1\,MB L2 cache, 6-wide pipeline, four FPUs, four IUs, and a 256-entry ROB
\end{itemize}

\subsection{Discussion}


In our first experiment, shown in Figure~\ref{fig:single} , runtimes without the accelerator increase sharply as $N$ approaches 64, whereas accelerator-enabled trials scale far more gracefully.
This single-core, single-accelerator comparison provides evidence that the accelerator delivers  performance gains, underscoring its potential for efficient deployment in HPC workloads.


%  algorithm  input_size  num_arrays    runtime
%4    biconj           4           0    3.80800
%1    biconj           8           0    3.92385
%7    biconj          16           0    4.91428
%5    biconj          32           0   29.69200
%2    biconj          64           0  225.94900

%0    biconj           4           1    3.77589
%6    biconj           8           1    3.83407
%3    biconj          16           1    4.12084
%8    biconj          32           1    7.39816
%9    biconj          64           1   32.27280

%9        cg           4           0    3.72803
%1        cg           8           0    3.77462
%5        cg          16           0    3.99011
%4        cg          32           0    5.14434
%8        cg          64           0   19.88550

%7        cg           4           1    3.70103
%0        cg           8           1    3.72746
%6        cg          16           1    3.88398
%3        cg          32           1    4.25867
%2        cg          64           1    5.57354


Across the architectural and CPU configurations, BiCGStab consistently required longer runtimes than CG.
The marginal performance gap between the small and large pipeline designs informs that \emph{I/O operations—not core or pipeline width—are the dominant bottleneck}, compounded by accelerator communication overhead.
In both solvers, runtime decreased as core count increased, demonstrating that performance scales with parallelism. More investigation must be looked into in this problem space.

\input{conclusion}



\section*{Acknowledgment}
This article has been authored by an employee of National Technology \& Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employee owns all right, title and interest in and to the article and is solely responsible for its contents. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan https://www.energy.gov/downloads/doe-public-access-plan.

\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\end{document}
